import numpy as np
import math

class MLP:

    """
    Constructor: Computes MLP.

    Args:
        input_size (int): size of input
        hiddenLayers_sizesizes ([int]): hidden layers and ots sizes.
        outputLayer (int): size of output layer
        seed (scalar): seed of the random numeric.
        epislom (scalar) : random initialization range. e.j: 1 = [-1..1], 2 = [-2,2]...
    """

    def __init__(self,input_size,hiden_layers, output_size, seed=0, epislom = 0.12):
        np.random.seed(seed)

        self.layer_sizes = [input_size] + hiden_layers + [output_size]
        self.n_layers_ = len(self.layer_sizes)

        self.thetas = []
        self.new_trained(self.thetas, epislom)
        
    """
    Reset the theta matrix created in the constructor by both theta matrix manualy loaded.

    Args:
        thetas ([array_like]): list of theta matrix to set in the neural network.
    """
    def new_trained(self,thetas, epislom=0.12):
        self.thetas = thetas
        # Inicializacion aleatoria de los pesos usando Xavier/Glorot
        '''
        Usamos esta tecnica porque ayuda a ajustar mejor los pesos iniciales, 
        evitando que sean demasiado grandes o pequeños, lo que puede causar problemas 
        como el desvanecimiento o explosión del gradiente.
        '''
        for i in range(self.n_layers_ - 1):
            in_size = self.layer_sizes[i]
            out_size = self.layer_sizes[i + 1]
            # Xavier initialization para sigmoid
            limit = np.sqrt(6.0 / (in_size + out_size))
            theta = np.random.uniform(-limit, limit, (out_size, in_size + 1))
            self.thetas.append(theta)
        
    """
    Num elements in the training data. (private)

    Args:
        x (array_like): input data. 
    """
    def _size(self,x):
        return x.shape[0]
    
    """
    Computes de sigmoid function of z (private)

    Args:
        z (array_like): activation signal received by the layer.
    """
    def _sigmoid(self,z):
        return 1 / (1 + np.exp(-z))

    """
    Computes de sigmoid derivation of de activation (private)

    Args:
        a (array_like): activation received by the layer.
    """   
    def _sigmoidPrime(self,a):
        return  a * (1 - a)

    """
    Run the feedwordwar neural network step

    Args:
        x (array_like): input of the neural network.

	Return 
	------
	activations (array_like): activation values of each layers
    zs (array_like): linear activation values of each layers
    """
    def feedforward(self,x):
        # Capa de entrada.
        activations = []  # Lista para almacenar las activaciones de cada capa.
        zs = [] # Lista para almacenar los valores z de cada capa.
        activations.append(np.hstack([np.ones((x.shape[0], 1)), x]))  # Anyadimos bias a la capa de entrada.

        # Iteramos por cada capa.
        for theta in self.thetas:
            z = activations[-1] @ theta.T  # Calculamos la senyal de activacion z.
            zs.append(z)
            # Calculamos la activacion de la capa siguiente.
            a = self._sigmoid(z)  
            # Calculamos la senyal de activacion z.
            if theta is not self.thetas[-1]:  # Si no es la capa de salida, anyadimos bias.
                a = np.hstack([np.ones((a.shape[0], 1)), a])
            activations.append(a)

        # Devolvemos a parte de las activaciones, los valores sin ejecutar la funcion de activacion.
        return activations, zs

    """
    Computes only the cost of a previously generated output (private)

    Args:
        yPrime (array_like): output generated by neural network.
        y (array_like): output from the dataset
        lambda_ (scalar): regularization parameter

	Return 
	------
	J (scalar): the cost.
    """
    def compute_cost(self, yPrime,y, lambda_):
        # Numero de ejemplos.
        m = y.shape[0]
        # Clipping para evitar log(0)
        epsilon = 1e-15
        yPrime = np.clip(yPrime, epsilon, 1 - epsilon)
        # Funcion de coste.
        J = (-1/m) * np.sum( y * np.log(yPrime) + (1 - y) * np.log(1 - yPrime) )
        return J + self._regularizationL2Cost(m, lambda_)
    

    """
    Get the class with highest activation value

    Args:
        X (array_like): output of the neural network.

	Return 
	------
	p (scalar): the class index with the highest activation value.
    """
    def predict(self, X):
        # Realizar forward propagation
        activations, zs = self.feedforward(X)
        # Obtener las probabilidades de salida (última capa de activaciones)
        output_probs = activations[-1]  # shape: (m, num_classes)
        # Obtener el índice de la clase con mayor probabilidad para cada muestra
        predictions = np.argmax(output_probs, axis=1)  # shape: (m,)
        return predictions
    
    """
    Compute the deltas of each layer (private)
    Args:
        activations (array_like): activations of each layer.
        y (array_like): output of the neural network.
    Return
    ------
    deltas: the delta matrix of each layer.
    """
    def _compute_deltas(self, activations, y):
        y_pred = activations[-1]
        deltas = [None] * (self.n_layers_ - 1) # Lista para almacenar los deltas de cada capa.
        deltas[-1] = y_pred - y  # Delta de la capa de salida, (outputError)

        for l in reversed(range(len(self.thetas) - 1)):
            delta = deltas[l + 1] @ self.thetas[l + 1][:, 1:]
            a = activations[l + 1][:, 1:]   # quitamos bias
            delta *= self._sigmoidPrime(a)
            deltas[l] = delta

        return deltas

    """
    Compute the gradients of both theta matrix parámeters and cost J (private)
    Args:
        activations (array_like): activations of each layer.
        deltas (array_like): deltas of each layer.
        lambda_ (scalar): regularization.
        m (scalar): the size of the X examples.
    Return
    ------
    grads: the gradient matrix of each layer.
    """
    def _compute_layer_gradients(self, activations, deltas, lambda_, m):
        grads = []
        for l in range(len(self.thetas)):
            a = activations[l]
            # Anyadimos bias a la activacion si no lo tiene.
            if a.shape[1] + 1 == self.thetas[l].shape[1]:
                a = np.hstack([np.ones((m, 1)), a])
            grad = (1/m) * (deltas[l].T @ a)
            # Regularizacion L2.
            grad += self._regularizationL2Gradient(self.thetas[l], lambda_, m)
            grads.append(grad)
        return grads
    """
    Compute the gradients of both theta matrix parámeters and cost J

    Args:
        x (array_like): input of the neural network.
        y (array_like): output of the neural network.
        lambda_ (scalar): regularization.

	Return 
	------
	J: cost
    grads: the gradient matrix of each layer.
    """
    def compute_gradients(self, x, y, lambda_):
        # Numero de ejemplos.
        m = self._size(x)
        # Forward propagation.
        activations, zs = self.feedforward(x)

        # Calcular el coste.
        J = self.compute_cost(activations[-1], y, lambda_)
        # Calculamos los deltas de las capas ocultas.
        deltas = self._compute_deltas(activations, y)
        # Calculamos los gradientes.
        grads = self._compute_layer_gradients(activations, deltas, lambda_, m)

        return J, grads
    
    """
    Compute L2 regularization gradient

    Args:
        theta (array_like): a theta matrix to calculate the regularization.
        lambda_ (scalar): regularization.
        m (scalar): the size of the X examples.

	Return 
	------
	L2 Gradient value
    """
    def _regularizationL2Gradient(self, theta, lambda_, m):
        # Crear una copia de theta multiplicada por (lambda/m).
        reg_grad = (lambda_ / m) * theta
        # No regularizar el bias (primera columna = 0).
        reg_grad[:, 0] = 0
        return reg_grad
    
    
    """
    Compute L2 regularization cost

    Args:
        lambda_ (scalar): regularization.
        m (scalar): the size of the X examples.

	Return 
	------
	L2 cost value
    """
    def _regularizationL2Cost(self, m, lambda_):
        reg = 0
        # Sumamos los cuadrados de todos los pesos excepto los bias (primera columna).
        for theta in self.thetas:
            reg += np.sum(theta[:, 1:]**2)
        # Calculamos el coste de regularizacion.
        reg_cost = (lambda_ / (2 * m)) * reg
        return reg_cost

    """
    Train the neural network using backpropagation
    Args:
        x (array_like): input of the neural network.
        y (array_like): output of the neural network.
        alpha (scalar): learning rate.
        lambda_ (scalar): regularization.
        numIte (int): number of iterations.
        verbose (int): verbosity level for logging.
    Return
    ------
    Jhistory (list): history of cost values over iterations.
    """
    def backpropagation(self, x, y, alpha, lambda_, numIte, verbose=0):
        Jhistory = []
        for i in range(numIte):
            # Calcular gradientes y coste
            J, grads = self.compute_gradients(x, y, lambda_)
            # Actualizar pesos (Descenso de gradiente)
            self.thetas = [
                theta - alpha * grad 
                for theta, grad in zip(self.thetas, grads) # Actualizamos cada theta con su gradiente correspondiente.
            ]
            # Guardar el coste en el historial.
            Jhistory.append(J)
            if verbose and i% verbose == 0:
                print(f"Iteration {(i+1):6}: Cost {float(J):8.4f} ")
        
        return Jhistory
