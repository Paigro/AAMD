import numpy as np
import math

class MLP:

    """
    Constructor: Computes MLP.

    Args:
        inputLayer (int): size of input
        hiddenLayer (int): size of hidden layer.
        outputLayer (int): size of output layer
        seed (scalar): seed of the random numeric.
        epislom (scalar) : random initialization range. e.j: 1 = [-1..1], 2 = [-2,2]...
    """

    def __init__(self,inputLayer,hidenLayer, outputLayer, seed=0, epislom = 0.12):
        np.random.seed(seed)
        self.inputLayer = inputLayer
        self.hidenLayer = hidenLayer
        self.outputLayer = outputLayer
        self.epislom = epislom

        # Inicializacion aleatoria de los pesos.
        theta1 = np.random.uniform(-self.epislom, self.epislom, (self.hidenLayer, self.inputLayer + 1))
        theta2 = np.random.uniform(-self.epislom, self.epislom, (self.outputLayer, self.hidenLayer + 1))
        # Asignamos las thetas a los atributos de la clase.
        self.new_trained(theta1,theta2)
        """
    Reset the theta matrix created in the constructor by both theta matrix manualy loaded.

    Args:
        theta1 (array_like): Weights for the first layer in the neural network.
        theta2 (array_like): Weights for the second layer in the neural network.
    """
    def new_trained(self,theta1,theta2):
        self.theta1 = theta1
        self.theta2 = theta2
        
    """
    Num elements in the training data. (private)

    Args:
        x (array_like): input data. 
    """
    def _size(self,x):
        return x.shape[0]
    
    """
    Computes de sigmoid function of z (private)

    Args:
        z (array_like): activation signal received by the layer.
    """
    def _sigmoid(self,z):
        return 1 / (1 + np.exp(-z))

    """
    Computes de sigmoid derivation of de activation (private)

    Args:
        a (array_like): activation received by the layer.
    """   
    def _sigmoidPrime(self,a):
        return  a * (1 - a)

    """
    Run the feedwordwar neural network step

    Args:
        x (array_like): input of the neural network.

	Return 
	------
	a1,a2,a3 (array_like): activation functions of each layers
    z2,z3 (array_like): signal fuction of two last layers
    """
    def feedforward(self,x):
        # Capa de entrada.
        m = self._size(x) # Numero de ejemplos.
        a1 = x
        # Datos de entrada originales.
        a1 = np.hstack([np.ones((m,1)),a1]) # Anyadimos una columna de unos a los datos de entrada (bias).

        # Procesamos capa oculta.
        # Calculamos la salida de la primera capa oculta.
        z2 = np.dot(a1,self.theta1.T) # Producto punto entre la entrada y los pesos de la primera capa.
        a2 = self._sigmoid(z2) # Aplicamos la funcion de activacion.
        m = self._size(a2) # Actualizamos el numero de ejemplos.
        a2 = np.hstack([np.ones((m,1)),a2])  # Anyadimos una columna de unos a la salida de la capa oculta (bias).

        # Procesamos capa de salida.
        z3 = np.dot(a2,self.theta2.T) # Producto punto entre la salida de la capa oculta y los pesos de la segunda capa.
        a3 = self._sigmoid(z3) # Aplicamos la funcion de activacion.
         
        # Devolvemos a parte de las activaciones, los valores sin ejecutar la funcion de activacion.
        return a1,a2,a3,z2,z3 

    """
    Computes only the cost of a previously generated output (private)

    Args:
        yPrime (array_like): output generated by neural network.
        y (array_like): output from the dataset
        lambda_ (scalar): regularization parameter

	Return 
	------
	J (scalar): the cost.
    """
    def compute_cost(self, yPrime,y, lambda_): # es una funci칩n interna por eso empieza por _
        # Numero de ejemplos.
        m = self._size(y)
        # Funcion de coste.
        J = (-1/m) * np.sum( 
            y * np.log(yPrime) + (1 - y) * np.log(1 - yPrime) 
            )
        return J + self._regularizationL2Cost(m, lambda_)
    

    """
    Get the class with highest activation value

    Args:
        a3 (array_like): output generated by neural network.

	Return 
	------
	p (scalar): the class index with the highest activation value.
    """
    def predict(self,a3):
        # axis=1 indica que se busque el maximo por fila (por ejemplo de entrenamiento).
        p = np.argmax(a3, axis=1)
        return p
    

    """
    Compute the gradients of both theta matrix par치meters and cost J

    Args:
        x (array_like): input of the neural network.
        y (array_like): output of the neural network.
        lambda_ (scalar): regularization.

	Return 
	------
	J: cost
    grad1, grad2: the gradient matrix (same shape than theta1 and theta2)
    """
    def compute_gradients(self, x, y, lambda_):
        # Numero de ejemplos.
        m = self._size(x)
        # Forward propagation.
        a1, a2, a3, z2, z3 = self.feedforward(x)
        # Calcular el coste.
        J = self.compute_cost(a3, y, lambda_)
        
        # Backpropagation.
        # Error en la capa de salida (delta3).
        delta3 = a3 - y  # (m x outputLayer).
        # Error en la capa oculta (delta2).
        # delta2 = (delta3 * theta2) .* sigmoidPrime(z2).
        # Eliminamos el error del bias (primera columna).
        delta2 = (delta3 @ self.theta2[:, 1:]) * self._sigmoidPrime(self._sigmoid(z2))  # (m x (hiddenLayer+1)).

        # Acumular los gradientes.
        grad1 = (1/m) * (delta2.T @ a1)  # (hiddenLayer x (inputLayer+1))
        grad2 = (1/m) * (delta3.T @ a2)  # (outputLayer x (hiddenLayer+1))
        
        # Anyadir regularizacion L2 a los gradientes.
        grad1 += self._regularizationL2Gradient(self.theta1, lambda_, m)
        grad2 += self._regularizationL2Gradient(self.theta2, lambda_, m)
        return (J, grad1, grad2)
    
    """
    Compute L2 regularization gradient

    Args:
        theta (array_like): a theta matrix to calculate the regularization.
        lambda_ (scalar): regularization.
        m (scalar): the size of the X examples.

	Return 
	------
	L2 Gradient value
    """
    def _regularizationL2Gradient(self, theta, lambda_, m):
        # Crear una copia de theta multiplicada por (lambda/m).
        reg_grad = (lambda_ / m) * theta
        # No regularizar el bias (primera columna = 0).
        reg_grad[:, 0] = 0
        return reg_grad
    
    
    """
    Compute L2 regularization cost

    Args:
        lambda_ (scalar): regularization.
        m (scalar): the size of the X examples.

	Return 
	------
	L2 cost value
    """
    def _regularizationL2Cost(self, m, lambda_):
         # Sumamos los cuadrados de todos los pesos excepto los bias (primera columna).
        sum_theta1_squared = np.sum(self.theta1[:, 1:]**2)
        sum_theta2_squared = np.sum(self.theta2[:, 1:]**2)
        # Calculamos el coste de regularizacion.
        reg_cost = (lambda_ / (2 * m)) * (sum_theta1_squared + sum_theta2_squared)
        return reg_cost
    
    def backpropagation(self, x, y, alpha, lambda_, numIte, verbose=0):
        Jhistory = []
        for i in range(numIte):
            # Calcular gradientes y coste
            J, grad1, grad2 = self.compute_gradients(x, y, lambda_)
            # Actualizar pesos (Descenso de gradiente)
            self.new_trained(self.theta1 - alpha * grad1, self.theta2 - alpha * grad2)
            
            Jhistory.append(J)
            if verbose > 0 :
                if i % verbose == 0 or i == (numIte-1):
                    print(f"Iteration {(i+1):6}: Cost {float(J):8.4f}   ")
        
        return Jhistory
    


"""
target_gradient funcit칩n of gradient test 1
"""
def target_gradient(input_layer_size,hidden_layer_size,num_labels,x,y,reg_param):
    mlp = MLP(input_layer_size,hidden_layer_size,num_labels)
    J, grad1, grad2 = mlp.compute_gradients(x,y,reg_param)
    return J, grad1, grad2, mlp.theta1, mlp.theta2


"""
costNN funcit칩n of gradient test 1
"""
def costNN(Theta1, Theta2,x, ys, reg_param):
    mlp = MLP(x.shape[1],1, ys.shape[1])
    mlp.new_trained(Theta1,Theta2)
    J, grad1, grad2 = mlp.compute_gradients(x,ys,reg_param)
    return J, grad1, grad2


"""
mlp_backprop_predict 2 to be execute test 2
"""
def MLP_backprop_predict(X_train,y_train, X_test, alpha, lambda_, num_ite, verbose):
    mlp = MLP(X_train.shape[1],25,y_train.shape[1])
    Jhistory = mlp.backpropagation(X_train,y_train,alpha,lambda_,num_ite,verbose)
    a3= mlp.feedforward(X_test)[2]
    y_pred=mlp.predict(a3)
    return y_pred